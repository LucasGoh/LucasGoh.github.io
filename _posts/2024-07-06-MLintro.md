---
title: ML fundamental
categories: [Math, ML]
math: true
---

This study note is a summary of [Neural Networks and Deep Learning](https://neuralnetworksanddeeplearning.com/)  , [3blue1brown neural networks](https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)


# Notations
$w_1,w_2,...$ means weights

b means bias

$x_1,x_2,...,$ means input , a - input or output

z means $wx+b$

$\sigma$ - logistic function - $\sigma(\mathrm{z}) = \frac{1}{1+\mathrm{e}^{-\mathrm{z}}} =\frac{1}{1+\exp \left(-\Sigma_j w_j x_j-b\right)}$

Δoutput is well approximated by
$$\begin{equation}
\Delta \text { output } \approx \sum_j \frac{\partial \text { output }}{\partial w_j} \Delta w_j+\frac{\partial \text { output }}{\partial b} \Delta b, \tag{5}
\end{equation}$$

# The architecture of neural networks

## cost function
$$\begin{equation}
C(w, b) \equiv \frac{1}{2 n} \sum_x\|y(x)-a\|^2 \tag{6}
\end{equation}$$

We'll call *C* the *quadratic* cost function; it's also sometimes known as the *mean squared error* or just *MSE*

### move the ball a small amount of $\Delta v_1$
$$\begin{equation}
\Delta C \approx \frac{\partial C}{\partial v_1} \Delta v_1+\frac{\partial C}{\partial v_2} \Delta v_2  \approx \nabla C\cdot \Delta v\tag{7}
\end{equation}$$

gradient vector by $\nabla C$
$$\begin{equation}
\nabla C \equiv\left(\frac{\partial C}{\partial v_1}, \frac{\partial C}{\partial v_2}\right)^T \tag{8}
\end{equation}$$

suppose we choose $\Delta v=-\eta\nabla C$
The equation (9) tells us that $\Delta C \approx-\eta \nabla C \cdot \nabla C=-\eta\|\nabla C\|^2$

$$-\Delta C(...)= C\left(w_0, w_1, \ldots, w_{13,001}\right)$$
> MSE not doing so well when $C(w,b)$ is large

> don't want $\eta$ to be too small, cause gradient descent algorithm will work very slowly.


# Compute second partial derivatives of C

## gradient descent
when we talk about gradient descent, we don't just care about whether each component should get nudges up or down, we care about which ones give you the most bang for you buck.

## stochastic gradient descent

Essential : randomly shuffle then make mini-batches

```
It's like drunken man, walk down fast then careful men slowly walking down
```
this can be quite costly, so use stochastic gradient descent
stochastic gradient descent can be used to speed up learning.
get $\nabla C_{x_j}$
$$\begin{equation}
\frac{\sum_{j=1}^m \nabla C_{X_j}}{m} \approx \frac{\sum_x \nabla C_x}{n}=\nabla C, \tag{18} \\
\end{equation}$$


The stochastic gradient descent works by picking out a randomly chosen mini-batch of training inputs, and training with those,

\begin{equation}
w_k \rightarrow w_k^{\prime}=w_k-\frac{\eta}{m} \sum_j \frac{\partial C_{X_j}}{\partial w_k} \tag{20}
\end{equation}
$$\begin{equation}
b_l \rightarrow b_l^{\prime}=b_l-\frac{\eta}{m} \sum_j \frac{\partial C_{X_j}}{\partial b_l}, \tag{21}
\end{equation}$$

